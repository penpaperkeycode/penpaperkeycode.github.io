---
layout: archive
title: ""
permalink: /cv/
author_profile: true
redirect_from:
  - /resume
---

{% include base_path %}

Work Experience
======
* NAVER Cloud - Technical Lead, AI Researcher
  * 2021.12 - Present @ Foundation Research Team
  * Pre-training efficiency & Transformers architecture
  * Multi-linguality and multi-modality of large-scale Transformers
* LG Energy Solution - Research Scientist
  * 2020.11 - 2021.11 @ Business Strategy Division
* Nepes - Assistant Manager
  * 2018.01 - 2020.10 @ Future Intelligence Division
  * Alternative military service program as an researcher  

Publications 
======
*equal contribution

### Lead author
1. **Jeonghoon Kim**, Byeongchan Lee, Cheonbok Park, Yeontaek Oh, Beomjun Kim, Taehwan Yoo, Seongjin Shin, Dongyoon Han, Jinwoo Shin, Kang Min Yoo. “Peri-LN: Revisiting Normalization Layer in the Transformer Architecture.” _ICML 2025 (Poster)_.
2. Jung Hyun Lee*, **Jeonghoon Kim***, June Yong Yang, Se Jung Kwon, Eunho Yang, Dongsoo Lee. “LRQ: Optimizing Post-Training Quantization for Large Language Models by Learning Low-Rank Weight-Scaling Matrices.” _NAACL 2025 (Poster)_.
3. Jung Hwan Heo*, **Jeonghoon Kim***, Beomseok Kwon, Byeongwook Kim, Se Jung Kwon, Dongsoo Lee. “Rethinking Channel Dimensions to Isolate Outliers for Low-bit Weight Quantization of Large Language Models.” _ICLR2024 (Poster)_.
4. **Jeonghoon Kim***, Jung Hyun Lee*, Sungdong Kim, Joonsuk Park, Kang Min Yoo, Se Jung Kwon, Dongsoo Lee. “Memory-Efficient Fine-Tuning of Compressed Large Language Models via sub-4-bit Integer Quantization.”  _NeurIPS 2023 (Poster)_.
5. Jung Hyun Lee*, **Jeonghoon Kim***, Se Jung Kwon, Dongsoo Lee. “FlexRound: Learnable Rounding based on Element-wise Division for Post-Training Quantization.” _ICML 2023 (Poster)_.
6. Cheonbok Park*, **Jeonghoon Kim***, Joosung Lee, Sanghwan Bae, Jaegul Choo, Kang Min Yoo. “Cross-lingual Collapse: How Language-Centric Foundation Models Shape Reasoning in Large Language Models.” _Arxiv (under review)_.


### Co-author
7. Gunho Park, Baeseong Park, Minsub Kim, Sungjae Lee, **Jeonghoon Kim**, Beomseok Kwon, Se Jung Kwon, Byeongwook Kim, Youngjoo Lee, Dongsoo Lee.  "LUT-GEMM: Quantized Matrix Multiplication based on LUTs for Efficient Inference in Large-Scale Generative Language Models." _ICLR2024 (Poster)_.
8. Yulhwa Kim, Jaeyong Jang, Jehun Lee, Jihoon Park, **Jeonghoon Kim**, Byeongwook Kim, Baeseong park, Se Jung Kwon, Dongsoo Lee, Jae-joon Kim. “Winning Both the Accuracy of Floating Point Activation and the Simplicity of Integer Arithmetic.” _ICLR 2023 (Poster)_.
9. Se Jung Kwon, **Jeonghoon Kim**, Jeongin Bae, Kang Min Yoo, Jin-Hwa Kim, Baeseong Park, Byeongwook Kim, Jung-Woo Ha, Nako Sung, Dongsoo Lee. “AlphaTuning: Quantization-Aware Parameter-Efficient Adaptation of Large-Scale Pre-Trained Language Models.” _Findings of EMNLP 2022_.
10. **HyperCLOVA X Team**. “HyperCLOVA X Technical Report” _Arxiv (Technical report)_.
11. Hyunseok Lee, **Jeonghoon Kim**, Beomjun Kim, Jihoon Tack, Chansong Jo, Jaehong Lee, Cheonbok Park, Sookyo In, Jinwoo Shin, Kang Min Yoo. “ReGUIDE: Data Efficient GUI Grounding via Spatial Reasoning and Search.” _Arxiv (under review)_.
12. **HyperCLOVA X Team**. “HyperCLOVA X THINK Technical Report” _Arxiv (Technical report)_.



Reviewing
======
* International Conference on Learning Representations
* Neural Information Processing Systems
* International Conference on Learning Representations
* Association for Computational Linguistics. 

Education
======
* Ph.D Student in Artificial Intelligence, Korea Advanced Institute of Science and Technology (KAIST), WIP
* M.S. in Control & Robotics Systems, Korea University, 2018

