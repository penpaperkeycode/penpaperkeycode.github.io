---
permalink: /
title: "About"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

> “Experience feeds us, opportunity grows us, relationships inspire us, and the world shapes us.”

I am a research scientist in Artificial Intelligence, focused on pushing the limits of <code style="color : magenta">efficiency</code>—from efficiency-aware pre-training schemes and algorithmic sparsity to state-of-the-art model compression—so that AI can deliver measurable, positive impact for society. Guided by rigorous empirical methodology and an appetite for interdisciplinary collaboration, I aim to translate cutting-edge research into deployable systems that make the best use of every flops, byte, and human interaction.

In a life lived only once, I am committed to building a career I will look back on without regret—one defined by curiosity, integrity, and lasting contribution.

-------

Work Experience
======
**NAVER Cloud** : Dec.2021 - Present / Foundation Research, Technical Lead (Research Scientist)
   * Pre-training efficiency & Transformers architecture
   * HyperCLOVA X modeling & pre-training (Technical Report: [2024](https://arxiv.org/abs/2404.01954), [2025](https://arxiv.org/abs/2506.22403))
   * Multi-linguality and multi-modality of large-scale Transformers
   * Model compression & quantization
   * Alongside my full-time role at NAVER, I am pursuing a Ph.D. in AI at [KAIST](https://www.kaist.ac.kr/en/) under Prof. [Jinwoo Shin](https://alinlab.kaist.ac.kr/shin.html), supported by a NAVER Cloud grant. 
     
**LG Energy Solution** : Nov.2020 - Nov.2021 / Business Strategy, Research Scientist
  
**Nepes** : Jan.2018 - Oct.2020 / Future Intelligence, Research Scientist
   * Alternative military service program as an researcher  

-------

Publications 
======
*equal contribution

### Lead author
1. [Peri-LN: Revisiting Normalization Layer in the Transformer Architecture.](https://arxiv.org/abs/2502.02732)  
**Jeonghoon Kim**, Byeongchan Lee, Cheonbok Park, Yeontaek Oh, Beomjun Kim, Taehwan Yoo, Seongjin Shin, Dongyoon Han, Jinwoo Shin, Kang Min Yoo.  
<code style="color : darkorange">ICML 2025</code>.
2. [LRQ: Optimizing Post-Training Quantization for Large Language Models by Learning Low-Rank Weight-Scaling Matrices.](https://arxiv.org/abs/2407.11534)  
Jung Hyun Lee*, **Jeonghoon Kim** *, June Yong Yang, Se Jung Kwon, Eunho Yang, Dongsoo Lee.  
  <code style="color : darkorange">NAACL 2025</code>.  
3. [Rethinking Channel Dimensions to Isolate Outliers for Low-bit Weight Quantization of Large Language Models.](https://arxiv.org/abs/2309.15531)  
Jung Hwan Heo*, **Jeonghoon Kim** *, Beomseok Kwon, Byeongwook Kim, Se Jung Kwon, Dongsoo Lee.    
<code style="color : darkorange">ICLR2024</code>.  
4. [Memory-Efficient Fine-Tuning of Compressed Large Language Models via sub-4-bit Integer Quantization.](https://arxiv.org/abs/2305.14152)  
**Jeonghoon Kim** *, Jung Hyun Lee*, Sungdong Kim, Joonsuk Park, Kang Min Yoo, Se Jung Kwon, Dongsoo Lee.   
  <code style="color : darkorange">NeurIPS 2023</code>.  
5. [FlexRound: Learnable Rounding based on Element-wise Division for Post-Training Quantization.](https://arxiv.org/abs/2306.00317)  
Jung Hyun Lee*, **Jeonghoon Kim** *, Se Jung Kwon, Dongsoo Lee.    
<code style="color : darkorange">ICML 2023</code>.  
6. [Cross-lingual Collapse: How Language-Centric Foundation Models Shape Reasoning in Large Language Models.](https://arxiv.org/abs/2506.05850)  
Cheonbok Park*, **Jeonghoon Kim** *, Joosung Lee, Sanghwan Bae, Jaegul Choo, Kang Min Yoo.  
<code style="color : darkorange">Arxiv (under review, 2025)</code>.


### Co-author
7. [LUT-GEMM: Quantized Matrix Multiplication based on LUTs for Efficient Inference in Large-Scale Generative Language Models.](https://arxiv.org/abs/2206.09557)  
Gunho Park, Baeseong Park, Minsub Kim, Sungjae Lee, **Jeonghoon Kim**, Beomseok Kwon, Se Jung Kwon, Byeongwook Kim, Youngjoo Lee, Dongsoo Lee.    
 <code style="color : darkorange">ICLR2024</code>.  
8. [Winning Both the Accuracy of Floating Point Activation and the Simplicity of Integer Arithmetic.](https://openreview.net/forum?id=z92lBy1ehjI)  
Yulhwa Kim, Jaeyong Jang, Jehun Lee, Jihoon Park, **Jeonghoon Kim**, Byeongwook Kim, Baeseong park, Se Jung Kwon, Dongsoo Lee, Jae-joon Kim.   
 <code style="color : darkorange">ICLR 2023</code>.
9. [AlphaTuning: Quantization-Aware Parameter-Efficient Adaptation of Large-Scale Pre-Trained Language Models.](https://arxiv.org/abs/2210.03858)  
Se Jung Kwon, **Jeonghoon Kim**, Jeongin Bae, Kang Min Yoo, Jin-Hwa Kim, Baeseong Park, Byeongwook Kim, Jung-Woo Ha, Nako Sung, Dongsoo Lee.   
 <code style="color : darkorange">Findings of EMNLP 2022</code>.
10. [HyperCLOVA X Technical Report](https://arxiv.org/abs/2404.01954)  
**HyperCLOVA X Team**.    
<code style="color : darkorange">Arxiv (Technical report, 2024)</code>.  
11. [ReGUIDE: Data Efficient GUI Grounding via Spatial Reasoning and Search.](https://arxiv.org/abs/2505.15259)  
Hyunseok Lee, **Jeonghoon Kim**, Beomjun Kim, Jihoon Tack, Chansong Jo, Jaehong Lee, Cheonbok Park, Sookyo In, Jinwoo Shin, Kang Min Yoo.  
 <code style="color : darkorange">Arxiv (under review, 2025)</code>.
12. [HyperCLOVA X THINK Technical Report](https://arxiv.org/abs/2506.22403)  
**HyperCLOVA X Team**.   
<code style="color : darkorange">Arxiv (Technical report, 2025)</code>.

-------

Reviewing
======
* International Conference on Machine Learning
* Neural Information Processing Systems
* International Conference on Learning Representations
* Association for Computational Linguistics. 
